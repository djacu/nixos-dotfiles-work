# Introduction

This blog will catalog my thoughts and experiences configuring NixOS. I have successfully installed NixOS with Flakes and Home Manager on a FrameWork laptop in the past. However, it was my first experience with NixOS and Nix, and I had no previous experience with functional languages. So most of the configuration was cobbled together from blog posts, videos, and other people's configuration files. Also, I was keeping most of this information in my head and never felt like I got a solid handle on what to do. It has been some time since then, but I do remember some things. So this experience will be like a soft reset versus starting from a cold boot.

# Goals

My initial goals for this will be:
* Use a ThinkPad X1 Gen8
* Install NixOS (obviously)
* Migrated the configuration to Flakes and Home Manager
* Install on an encrypted ZFS filesystem
* Use a 2FA scheme to decrypt the drive

Not a bad list to start with. I'll tackle understanding ZFS first before returning to NixOS. I've been looking at [shavee](https://github.com/ashuio/shavee), which I believe I can use to decrypt my ZFS filesystem at login. Reading through the README, I get a sense that ZFS is different that file systems I've used before.

# ZFS

## Learning About ZFS

### Structure

There is an informative article on [arstechnica](https://arstechnica.com/information-technology/2020/05/zfs-101-understanding-zfs-storage-and-performance/). It says that the uppermost ZFS structure is a `zpool`. Each `zpool` can have one or more `vdevs` which contain one or more `devices`. ZFS redundancy is at the `vdev` level which I think I am not too concerned with as this is for a single drive on a laptop. Although, it says if any storage `vdev` or `SPECIAL vdev` is lost, then the entire `zpool` is lost with it. There are other `vdev`'s, `CACHE` and `LOG`, that can be lost without loosing the whole `zpool`. Okay this sort of makes sense. Okay so `vdev` is where you would setup a RAID or mirror configuration. `device` is where you get down to bare metal or any random-access block device.

Skipping a few levels there are `datasets`, `blocks`, and `sectors`. It says that a ZFS `dataset` is roughly analogous to a standard, mounted filesystem. Although, you can put quotas on each `dataset` so they have a cap on the amount of data that can be stored. A `dataset` can be the child of a `zpool` or another `dataset` and will inherit properties from a parent `dataset` unless explicitly overridden. Each `dataset` can be mounted at a system mount point and altered. All data -- including metadata -- is stored in `blocks`. The maximum size of a `block` is defined for each `dataset` in the `recordsize` property. The minimum size is defined using the `ashift` property where the value is 2^ashift^ (2^12^ is 4096 byte sectors). ***Do not set*** `ashift` ***to low!*** Setting the minimum sector size, via `ashift`, below the physical media sector size ***will*** result in significant performance loss. Additionally, `ashift` is ***immutable*** so take care when creating a new `zpool` or `vdev`. ZFS tries to query the physical disk for its sector size in order to set `ashift` properly, but many disks can lie, so it is advised to set `ashift` manually. It is also recommended to set `ashift` to 12 or 13 for future-proofing.

### Features

Copy-on-Write (CoW) is built in. I didn't realize how it worked when asked to modify a file in-place. The unlinking of the old block and linking of the new block is nice. It protects your data from corruption during system failure or unhandled power loss at the disk management level. Even the RAID configurations are safe.

CoW enables ZFS to have atomic snapshots and incremental asynchronous replication. [Sanoid](https://github.com/jimsalterjrs/sanoid/) can be used for *fast* incremental replication that is significantly faster than rsync.

Inline compression and Adaptive Replacement Cache are neat but not super relevant for getting a system running. Although, it should be noted that each compression algorithm has use cases where it excels over the others. For example, ZLE might be a better choice for a `dataset` that is comprised entirely of incompressible data. However, LZ4 is good for nearly all use-cases and suffers a small performance penalty when encountering incompressible data.
